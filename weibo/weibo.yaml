# training settings
num_classes: 2 # options: 2, 3, 6
batch_size: 64 # try maxing out batch size to VRAM, 24Gb available
learning_rate: 1.0e-4
num_epochs: 30 # only one epoch needed for convergence
max_seq_len: 40
dropout_p: 0.1
gpus: [0]
num_cpus: 12 
data_path: "data/weibo/"

# OGM-GE settings
grad_mod_type: "OGM_GE" # Options: (None | 'OGM_GE' | 'OGM' | 'noise') 
alpha: 0.1 

# main settings that need to be checked
use_wandb: True
model_type: "ensemble" # Options: see __init__.py in this directory
group_name: "weibo_cls2_ensemble_baseline_augs_lr=1e-4_sampler" 
seed: 0
use_scheduler: True

# NOTE: each epoch takes about 25 minutes

